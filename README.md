In this project, I used the Cultural Consensus Theory model and PyMC to examine a plant knowledge dataset. A set of informants were asked questions about local plant information. For each informant’s competence D, I chose a Uniform distribution as its prior. I had initially selected a Beta distribution, because it models probabilities and is quite flexible, however I found that the r-hat values for every informant were well above 1.01, which indicated that there were issues with convergence.  After changing it to a Uniform distribution, all the r-hat values were precisly 1.0, meaning the chains converged successfully. I utilized a Bernoulli distribution for Z, since that was recommended in the instructions for this assignment. For the MCMC sample, I prompted it to do 2000 draws with 4 chains to ensure convergence, and implemented a tuning of 1000 as well. 

Based on the mean competence estimates, I can conclude that informant 3 is the least competent, with a mean of 0.561 (the lowest in the plot), while informant 6 is the most competent in their responses, with a mean of 0.872 (the highest in the plot). When it comes to the posterior means for every consensus answer, it’s apparent that PQ17 had the lowest mean of 0.007, while PQ20 had the largest posterior mean of 0.998. The model asserts that there's a 0.7% probability of Q17's answer being 1, and a 99.8% chance of Q20's answer being 1. Furthermore, the CCT model estimated that the general majority would vote Q2, Q3, Q5-Q11, Q14, and Q20 to be correct, however the actual naive aggregation challenges the model quite a bit. It visualizes the simple majority to declare Q3, Q5, Q7, Q9, Q11, and Q20 to be correct, which is significantly less than the CCT model’s estimation. Items 1, 5, 7, 9, and 13 were not aligned between the answer key and the actual simple majority vote. 
